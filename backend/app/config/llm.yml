# LLM configuration (model and parameters) for summarization and plugins
model_name: mistralai/magistral-small
model_temperature: 0.6
model_max_tokens: 16000
openai_api_base: "http://host.docker.internal:11434/v1"